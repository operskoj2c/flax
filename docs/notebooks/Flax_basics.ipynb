{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flax basics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyANAaZtbs86"
      },
      "source": [
        "# Setting up our environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdrEVv9tinJn",
        "outputId": "da197a11-3ede-47f9-f2a7-9f5beeb8b46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install the newest JAXlib version.\n",
        "!pip install --upgrade -q pip jax jaxlib\n",
        "# Install Flax at head:\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN6bZDaReZO2"
      },
      "source": [
        "import jax\n",
        "from typing import Any, Callable, Sequence, Optional\n",
        "from jax import lax, random, numpy as jnp\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "\n",
        "from jax.config import config\n",
        "config.enable_omnistaging() # Linen requires enabling omnistaging"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCCwAbOLiscA"
      },
      "source": [
        "# Linear regression with Flax\n",
        "\n",
        "In the previous *JAX for the impatient* notebook, we finished up with a linear regression example. As we know, linear regression can also be written as a single dense neural network layer, which we will show in the following so that we can compare how it's done.\n",
        "\n",
        "A dense layer is a layer that has a kernel parameter $W\\in\\mathcal{M}_{m,n}(\\mathbb{R})$ where $m$ is the number of features as an output of the model, and $n$ the size of the input, and a bias parameter $b\\in\\mathbb{R}^m$. The dense layers returns $Wx+b$ from an input $x\\in\\mathbb{R}^n$.\n",
        "\n",
        "This dense layer is already provided by Flax in the `flax.linen` module (here imported as `nn`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWX2zEtphT4Y"
      },
      "source": [
        "# We create one dense layer instance (taking 'features' parameter as input)\n",
        "model = nn.Dense(features=5)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmzP1QoQYAAN"
      },
      "source": [
        "Layers (and models in general, we'll use that word from now on) are subclasses of the `linen.Module` class.\n",
        "\n",
        "## Model parameters & initialization\n",
        "\n",
        "Contrarily to what you might expect coming from some other framework, parameters are not stored with the models themselves. At model creation, no parameters are initialized, and you need to do so by calling the `init` function manually to generate those, using a PRNGKey and a dummy input parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K529lhzeYtl8",
        "outputId": "8dbffc5d-7768-4c92-f843-a73a6c35fc24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "key1, key2 = random.split(random.PRNGKey(0))\n",
        "x = random.normal(key1, (10,)) # Dummy input\n",
        "init_params = model.init(key2, x) # Initialization call\n",
        "jax.tree_map(lambda x: x.shape, init_params) # Checking output shapes"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({'params': {'bias': (5,), 'kernel': (10, 5)}})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7Y9xMEewmO"
      },
      "source": [
        "*Note: JAX and Flax, like numpy are row-based systems, meaning that vectors a represented as row vectors and not column. This can be seen in the shape of all tensors we talk about.*\n",
        "\n",
        "Here we an see the result is what we expect: bias and kernel parameters of the correct size. What happens under the hood is:\n",
        "\n",
        "*   The dummy input variable `x` is used to trigger shape inference: we only declared the number of features we wanted in the output of the model, not the size of the input. Flax finds out by itself the correct size of the kernel.\n",
        "*   The random PRNG key is used (and needed!) to trigger the initialization functions (those have default values provided by the module here, but could have been replaced in the `model = nn.Dense()` call using `kernel_init` and `bias_init` keyword arguments.\n",
        "* Initalization functions are called to generate the intial set of parameters that the model will use. Those are functions that take as arguments `(PRNG Key, shape, dtype)` and return an Array of shape `shape`.\n",
        "* The init function returns the initalized set of parameter (you can also get the output of the evaluation on the dummy input with the same syntax but using the `init_with_output` method instead of `init`.\n",
        "\n",
        "As we mentionned, the parameters are never stored inside of the model itself. To evaluate the model with a given set of parameters, we just have to call the `apply` method by providing it the parameters to use as well as the input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8ietJecWiuK",
        "outputId": "972d0c12-741a-47e8-8558-c2e70aedda0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.apply(init_params, x) # Evaluating model at point x with params init_params"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-0.7358944,  1.3583755, -0.7976872,  0.8168598,  0.6297793],            dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVsjgYzuSBGL"
      },
      "source": [
        "##Â Gradient descent\n",
        "\n",
        "If you jumped here directly without going through the JAX part, here is the linear regression formulation we're going to use: from a set of data points $\\{(x_i,y_i), i\\in \\{1,\\ldots, k\\}, x_i\\in\\mathbb{R}^n,y_i\\in\\mathbb{R}^m\\}$, we try to find a set of parameters $W\\in \\mathcal{M}_{m,n}(\\mathbb{R}), b\\in\\mathbb{R}^m$ such that the function $f_{W,b}(x)=Wx+b$ minimizes the mean squared error:\n",
        "$$\\mathcal{L}(W,b)\\rightarrow\\frac{1}{k}\\sum_{i=1}^{k} \\frac{1}{2}\\|y_i-f_{W,b}(x_i)\\|^2_2$$\n",
        "(Note: this is a rough explanation, theoretically we should be minimizing the expectation of the loss, however for the sake of simplicity here we consider only the sampled loss).\n",
        "\n",
        "Here, we see that the tuple $(W,b)$ matches the parameters of the Dense layer. We'll perform gradient descent using those. Let's first generate the fake data we'll use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFIiMnL4dl-e",
        "outputId": "9f67796e-98be-4b09-f989-c2e193ed426c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set problem dimensions\n",
        "nsamples = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "key = random.PRNGKey(0)\n",
        "k1, k2 = random.split(key)\n",
        "W = random.normal(k1, (ydim, xdim))\n",
        "b = random.normal(k2, (ydim,))\n",
        "true_params = freeze({'params': {'bias': b, 'kernel': W.T}})\n",
        "\n",
        "# Generate samples with additional noise\n",
        "ksample, knoise = random.split(k1)\n",
        "x_samples = random.normal(ksample, (nsamples, xdim))\n",
        "y_samples = jax.vmap(lambda x:jnp.dot(W,x)+b)(x_samples) + 0.1*random.normal(knoise,(nsamples, ydim))\n",
        "print(\"x shape:\", x_samples.shape, \"; y shape:\", y_samples.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x shape: (20, 10) ; y shape: (20, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHkioicCiUbx"
      },
      "source": [
        "Now let's generate the loss function (mean squarred error) with that data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqJaVc7BeNyT"
      },
      "source": [
        "def make_mse(x_batched,y_batched):\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      pred = model.apply(params, x)\n",
        "      return jnp.inner(y-pred,y-pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result.\n",
        "\n",
        "# Get the sampled loss\n",
        "loss = make_mse(x_samples, y_samples)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKru__mi15v"
      },
      "source": [
        "And finally perform the gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePEl1ndse0Jq",
        "outputId": "d90deb86-1541-4c96-c7b4-a0bf29e62aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "params = init_params\n",
        "\n",
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', loss(true_params))\n",
        "grad_fn = jax.value_and_grad(loss)\n",
        "\n",
        "for i in range(101):\n",
        "  # We perform one gradient update\n",
        "  loss_val, grad = grad_fn(params)\n",
        "  params = jax.tree_multimap(lambda old,grad: old-alpha*grad, params, grad)\n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), loss_val)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss for \"true\" W,b:  0.023639774\n",
            "Loss step 0:  33.644146\n",
            "Loss step 10:  0.54844475\n",
            "Loss step 20:  0.1386285\n",
            "Loss step 30:  0.05103702\n",
            "Loss step 40:  0.02445298\n",
            "Loss step 50:  0.015832197\n",
            "Loss step 60:  0.01298588\n",
            "Loss step 70:  0.01204048\n",
            "Loss step 80:  0.011725718\n",
            "Loss step 90:  0.011620826\n",
            "Loss step 100:  0.011585844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqEnJ9Poyb6q"
      },
      "source": [
        "## Build-in optimization API\n",
        "\n",
        "Flax provides an optimization package in `flax.optim` to make your life easier when training models. The process is:\n",
        "\n",
        "1.   You choose an optimization method (e.g. `optim.GradientDescent`, `optim.Adam`)\n",
        "2.   From the previous optimization method, you create a wrapper around the parameters you're going to optimize for with the `create` method. Your parameters are accessible through the `target` field.\n",
        "3. You compute the gradients of your loss with `jax.value_and_grad()`.\n",
        "4. At every iteration, you compute the gradients at the current point, then use the `apply_gradient()` method on the optimizer to return a new optimizer with updated parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce77uDJx1bUF"
      },
      "source": [
        "from flax import optim\n",
        "optimizer_def = optim.GradientDescent(learning_rate=alpha) # Choose the method\n",
        "optimizer = optimizer_def.create(init_params) # Create the wrapping optimizer with initial parameters\n",
        "grad_fn = jax.value_and_grad(loss)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTSv0vx13xPO",
        "outputId": "e51f7e15-851b-4fef-b994-8c874d4a5645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(101):\n",
        "  loss_val, grad = grad_fn(optimizer.target)\n",
        "  optimizer = optimizer.apply_gradient(grad) #Â Return the updated optimizer with parameters.\n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), loss_val)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss step 0:  33.644146\n",
            "Loss step 10:  0.54844475\n",
            "Loss step 20:  0.1386285\n",
            "Loss step 30:  0.05103702\n",
            "Loss step 40:  0.02445298\n",
            "Loss step 50:  0.015832197\n",
            "Loss step 60:  0.01298588\n",
            "Loss step 70:  0.01204048\n",
            "Loss step 80:  0.011725718\n",
            "Loss step 90:  0.011620826\n",
            "Loss step 100:  0.011585844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eAPPwtpXYu7"
      },
      "source": [
        "## Serializing the result\n",
        "\n",
        "Now that we're happy with the result of our training, we might want to save the model parameters to load them back later. Flax provides a serialization package to enable you to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiUPRU93XnAZ",
        "outputId": "7dd6f749-d552-4356-c2d2-c793d12e41de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from flax import serialization\n",
        "bytes_output = serialization.to_bytes(optimizer.target)\n",
        "dict_output = serialization.to_state_dict(optimizer.target)\n",
        "print('Dict output')\n",
        "print(dict_output)\n",
        "print(\"Bytes output\")\n",
        "print(bytes_output)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dict output\n",
            "{'params': {'bias': DeviceArray([-1.4546485, -2.024733 ,  2.0802515,  1.2192372, -0.9952478],            dtype=float32), 'kernel': DeviceArray([[ 1.0113373 , -1.178727  ,  0.17029275, -0.38268968,\n",
            "               1.0006864 ],\n",
            "             [ 0.15007997,  0.29265866,  1.3902837 ,  1.7733499 ,\n",
            "              -0.6942389 ],\n",
            "             [ 0.01514493,  1.4338304 , -1.395063  ,  1.0604348 ,\n",
            "               1.0923973 ],\n",
            "             [-0.91484565,  0.12237039,  0.42970654, -0.59060466,\n",
            "              -1.8361856 ],\n",
            "             [ 0.32762164, -1.3771925 , -2.1653433 ,  1.0377196 ,\n",
            "              -0.44736037],\n",
            "             [ 1.734053  , -1.1486411 ,  0.5843343 ,  0.9914667 ,\n",
            "              -0.64042985],\n",
            "             [ 0.9330395 , -0.19898592,  0.8064922 , -1.2266037 ,\n",
            "               0.4537833 ],\n",
            "             [ 1.164198  ,  0.03010925,  0.34369385,  0.3231217 ,\n",
            "              -1.1663771 ],\n",
            "             [ 1.1287069 ,  1.3840592 ,  0.55565697,  0.7966899 ,\n",
            "              -0.7405516 ],\n",
            "             [-0.10766877,  0.07589053,  0.94861233, -1.16194   ,\n",
            "               0.17127322]], dtype=float32)}}\n",
            "Bytes output\n",
            "b'\\x81\\xa6params\\x82\\xa4bias\\xc7\\x1d\\x01\\x93\\x91\\x05\\xa3<f4\\xc4\\x14\\xec1\\xba\\xbf:\\x95\\x01\\xc0\\xd7\"\\x05@\\xf7\\x0f\\x9c?\\x8f\\xc8~\\xbf\\xa6kernel\\xc7\\xd2\\x01\\x93\\x92\\n\\x05\\xa3<f4\\xc4\\xc8\\x80s\\x81?\\x87\\xe0\\x96\\xbf9a.>\\xe7\\xef\\xc3\\xbe~\\x16\\x80?\\x90\\xae\\x19>[\\xd7\\x95>\\xd1\\xf4\\xb1?!\\xfd\\xe2?\\xa4\\xb91\\xbfo\"x<\\xc1\\x87\\xb7?m\\x91\\xb2\\xbfT\\xbc\\x87?\\xad\\xd3\\x8b?S3j\\xbfT\\x9d\\xfa=\\x7f\\x02\\xdc>\\xde1\\x17\\xbf!\\x08\\xeb\\xbf\\x06\\xbe\\xa7>\\xd8G\\xb0\\xbf\\xfc\\x94\\n\\xc0\\xff\\xd3\\x84?k\\x0c\\xe5\\xbes\\xf5\\xdd?\\xac\\x06\\x93\\xbf\\xef\\x96\\x15?\\xc3\\xd0}?6\\xf3#\\xbf\\xad\\xdbn?\\xf7\\xc2K\\xbeFvN?Z\\x01\\x9d\\xbfIV\\xe8>q\\x04\\x95?\\xab\\xa7\\xf6<\\xa4\\xf8\\xaf>5p\\xa5>\\xd8K\\x95\\xbfxy\\x90?\\xda(\\xb1?\\x89?\\x0e?\\xdf\\xf3K?\\xca\\x94=\\xbfr\\x81\\xdc\\xbd~l\\x9b=B\\xd8r?s\\xba\\x94\\xbf?b/>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eielPo2KZByd"
      },
      "source": [
        "**TODO: how do you keep the structure around ? Regenerated from model init ? What about filling another pattern that looks \"alike\" ?**\n",
        "\n",
        "To load the model back, you'll need to use as a template de model parameter structure, like the one you would get from the model initialization. Here we use the previously generated `init_params` as template. Note that this will produce a new variable structure, and not mutate in-place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOhoBDCOYYJ5",
        "outputId": "a8234ba8-989e-46a8-c728-c5970c6abe3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "serialization.from_bytes(init_params, bytes_output)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({'params': FrozenDict({'kernel': array([[ 1.0113373 , -1.178727  ,  0.17029275, -0.38268968,  1.0006864 ],\n",
              "       [ 0.15007997,  0.29265866,  1.3902837 ,  1.7733499 , -0.6942389 ],\n",
              "       [ 0.01514493,  1.4338304 , -1.395063  ,  1.0604348 ,  1.0923973 ],\n",
              "       [-0.91484565,  0.12237039,  0.42970654, -0.59060466, -1.8361856 ],\n",
              "       [ 0.32762164, -1.3771925 , -2.1653433 ,  1.0377196 , -0.44736037],\n",
              "       [ 1.734053  , -1.1486411 ,  0.5843343 ,  0.9914667 , -0.64042985],\n",
              "       [ 0.9330395 , -0.19898592,  0.8064922 , -1.2266037 ,  0.4537833 ],\n",
              "       [ 1.164198  ,  0.03010925,  0.34369385,  0.3231217 , -1.1663771 ],\n",
              "       [ 1.1287069 ,  1.3840592 ,  0.55565697,  0.7966899 , -0.7405516 ],\n",
              "       [-0.10766877,  0.07589053,  0.94861233, -1.16194   ,  0.17127322]],\n",
              "      dtype=float32), 'bias': array([-1.4546485, -2.024733 ,  2.0802515,  1.2192372, -0.9952478],\n",
              "      dtype=float32)})})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mNu8nuOhDC5"
      },
      "source": [
        "# Defining your own models\n",
        "\n",
        "Flax allows you to define your own models, which should be a bit more complicated than a linear regression. In this section, we'll show you how to build simple models. To do so, you'll need to create subclasses of the base `nn.Module` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sllHAdRlpmQ"
      },
      "source": [
        "## Module basics\n",
        "\n",
        "The base abstraction for models is the `nn.Module` class, and every type of predefined layers in Flax (like the previous `Dense`) is a subclass of `nn.Module`. Let's take a look and start by defining a simple, but custom multi-layer perceptron i.e. a sequence of Dense layer interleaved with calls to a non-linear activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbfrfbkxgPhg",
        "outputId": "b2b515d1-44cf-4bd0-ce52-01e37612abde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class ExplicitMLP(nn.Module):\n",
        "  features: Sequence[int]\n",
        "\n",
        "  def setup(self):\n",
        "    # we automatically know what to do with lists, dicts of submodules\n",
        "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
        "    # for single submodules, we would just write:\n",
        "    # self.layer1 = nn.Dense(self, feat1)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, lyr in enumerate(self.layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = ExplicitMLP(features=[3,4,5])\n",
        "init_variables = model.init(key2, x)\n",
        "y = model.apply(init_variables, x)\n",
        "\n",
        "print('initialized parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(init_variables)))\n",
        "print('output:\\n', y)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialized parameter shapes:\n",
            " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
            "output:\n",
            " [[ 4.2292815e-02 -4.3807115e-02  2.9323792e-02  6.5492536e-03\n",
            "  -1.7147182e-02]\n",
            " [ 1.2967804e-01 -1.4551792e-01  9.4432175e-02  1.2521386e-02\n",
            "  -4.5417294e-02]\n",
            " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00]\n",
            " [ 9.3024090e-04  2.7864411e-05  2.4478839e-04  8.1344356e-04\n",
            "  -1.0110775e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDITIjXitEZl"
      },
      "source": [
        "As we can see, a `nn.Module` subclass is made of:\n",
        "\n",
        "*   A collection of data fields (`nn.Module` are Python dataclasses) - here we only have the `features` field of type `Sequence[int]`.\n",
        "*   A `setup()` method that is being called at the end of the `__postinit__` where you can register submodules, variables, parameters you will need in your model.\n",
        "*   A `__call__` function that returns the output of the model from a given input.\n",
        "*   The model structure defines a pytree of parameters following the same tree structure as the model: the params tree contains one `layers_n` sub dict per layer, and each of those contain the parameters of the associated Dense layer. The layout is very explicit and fits your mental model of what you would expect.\n",
        "\n",
        "Since here we have a very simple model, we could have used an alternative (but equivalent) way of declaring the submodules inline in the `__call__` using the `@nn.compact` annotation like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTCbdpQ4suSK",
        "outputId": "a92c9628-6fbf-4155-a447-b74be9a91f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "  features: Sequence[int]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, feat in enumerate(self.features):\n",
        "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
        "      if i != len(self.features) - 1:\n",
        "        x = nn.relu(x)\n",
        "      # providing a name is optional though!\n",
        "      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n",
        "    return x\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = SimpleMLP(features=[3,4,5])\n",
        "init_variables = model.init(key2, x)\n",
        "y = model.apply(init_variables, x)\n",
        "\n",
        "print('initialized parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(init_variables)))\n",
        "print('output:\\n', y)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialized parameter shapes:\n",
            " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
            "output:\n",
            " [[ 4.2292815e-02 -4.3807115e-02  2.9323792e-02  6.5492536e-03\n",
            "  -1.7147182e-02]\n",
            " [ 1.2967804e-01 -1.4551792e-01  9.4432175e-02  1.2521386e-02\n",
            "  -4.5417294e-02]\n",
            " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00]\n",
            " [ 9.3024090e-04  2.7864411e-05  2.4478839e-04  8.1344356e-04\n",
            "  -1.0110775e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es7YHjgexT-L"
      },
      "source": [
        "There are however a few differences you should be aware of between the two declaration modes:\n",
        "\n",
        "*   In the `setup` method, you don't have access to the `input` parameter, meaning that you can't rely on shape inference, thus you need to define explicitely (often as a dataclass field) the shapes that you are missing from run time.\n",
        "*   In `setup`, you are able to name some sublayers and keep them around for further use (e.g. encoder/decoder methods in autoencoders).\n",
        "*   **TODO: Anything else? Like pros for using setup?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ykceROJyp7W"
      },
      "source": [
        "## Module parameters\n",
        "\n",
        "In the previous MLP example, we relied only on predefined layers and operators (`Dense`, `relu`). Let's imagine that you didn't have a Dense layer provided by Flax and you wanted to write it on your own. Here is what it would look like using the `@nn.compact` way to declare a new modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK371Pt_vVfR",
        "outputId": "d570f6e5-9166-41ed-da09-4d380c9d94b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class SimpleDense(nn.Module):\n",
        "  features: int\n",
        "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    kernel = self.param('kernel',\n",
        "                        self.kernel_init, # Initialization function\n",
        "                        (inputs.shape[-1], self.features))  # shape info.\n",
        "    y = lax.dot_general(inputs, kernel,\n",
        "                        (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
        "    bias = self.param('bias', self.bias_init, (self.features,))\n",
        "    y = y + bias\n",
        "    return y\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = SimpleDense(features=3)\n",
        "init_variables = model.init(key2, x)\n",
        "y = model.apply(init_variables, x)\n",
        "\n",
        "print('initialized parameters:\\n', init_variables)\n",
        "print('output:\\n', y)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialized parameters:\n",
            " FrozenDict({'params': {'kernel': DeviceArray([[ 0.6503669 ,  0.8678979 ,  0.46042678],\n",
            "             [ 0.05673932,  0.9909285 , -0.63536596],\n",
            "             [ 0.76134115, -0.3250529 , -0.6522163 ],\n",
            "             [-0.8243032 ,  0.4150194 ,  0.19405058]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}})\n",
            "output:\n",
            " [[ 0.5035518   1.8548559  -0.4270196 ]\n",
            " [ 0.0279097   0.5589246  -0.43061775]\n",
            " [ 0.35471284  1.5741     -0.3286552 ]\n",
            " [ 0.5264864   1.2928858   0.10089308]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKyhfzVpzC94"
      },
      "source": [
        "Here, we see how both declare and assign a parameter to the model using the `self.param` method. It takes as input `(name, init_fn, *init_args)` : \n",
        "\n",
        "*   `name` is simply the name of the parameter that will end up in the parameter structure.\n",
        "*   `init_fun` is a function with input `(PRNGKey, *init_args)` returning an Array with `init_args` the arguments needed to call the initialisation function\n",
        "*   `init_args` the arguments to provide to the initialization function.\n",
        "\n",
        "Such params can also be declared in the `setup` method, but as mentionned above, it won't be able to use shape inference as Flax is using lazy initialization at first call site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Te6MbLGErXJ",
        "outputId": "4c080943-eabf-456a-dba5-90f948af8ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# What happens when you use the mutable parameter? TODO\n",
        "model.apply(init_variables, x, mutable=['params'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[ 0.5035518 ,  1.8548559 , -0.4270196 ],\n",
              "              [ 0.0279097 ,  0.5589246 , -0.43061775],\n",
              "              [ 0.35471284,  1.5741    , -0.3286552 ],\n",
              "              [ 0.5264864 ,  1.2928858 ,  0.10089308]], dtype=float32),\n",
              " FrozenDict({'params': {'kernel': DeviceArray([[ 0.6503669 ,  0.8678979 ,  0.46042678],\n",
              "              [ 0.05673932,  0.9909285 , -0.63536596],\n",
              "              [ 0.76134115, -0.3250529 , -0.6522163 ],\n",
              "              [-0.8243032 ,  0.4150194 ,  0.19405058]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}}))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmSpxyqLDr58"
      },
      "source": [
        "## Variables and collections of variables\n",
        "\n",
        "As we've seen so far, working with models means working with:\n",
        "\n",
        "*   A subclass of `nn.Module`;\n",
        "*   A pytree of parameters for the model (typically from `model.init()`);\n",
        "\n",
        "However this is not enough to cover everything that we would need for machine learning, especially neural networks. In some cases, you might want your neural network to keep track of some internal state while it runs (e.g. batch normlization layers). There is a way to declare variables beyond the parameters of the model with the `variable` method.\n",
        "\n",
        "Let's start with a (useless) model that keeps track of the number of samples it has seen.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6_tR-nPzB1i",
        "outputId": "055f70aa-d295-4d87-ec25-fb40cfcfdc9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Counter(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    offset = self.param('offset', lambda rng,n: n, 3) # Dummy parameter as example\n",
        "    # easy pattern to detect if we're initializing\n",
        "    is_initialized = self.has_variable('counter', 'count')\n",
        "    counter = self.variable('counter', 'count', lambda: jnp.zeros((), jnp.int32))\n",
        "    if is_initialized:\n",
        "      counter.value += x\n",
        "    return counter.value + offset\n",
        "\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = 1.0\n",
        "model = Counter()\n",
        "init_variables = model.init(key1, x)\n",
        "print('initialized variables:\\n', init_variables)\n",
        "\n",
        "y, updated_state = model.apply(init_variables, x, mutable=['counter'])\n",
        "\n",
        "print('updated variables:\\n', updated_state)\n",
        "print('output:\\n', y)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialized variables:\n",
            " FrozenDict({'params': {'offset': 3}, 'counter': {'count': DeviceArray(0, dtype=int32)}})\n",
            "updated variables:\n",
            " FrozenDict({'counter': {'count': DeviceArray(1., dtype=float32)}})\n",
            "output:\n",
            " 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OHBbMJng3ic"
      },
      "source": [
        "Here, `updated_variables` returns only the state variables that are being mutated by the model while applying it on data. To update the variables and get the new parameters of the model, we can use the following pattern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbTsCAvZcdBy",
        "outputId": "73662db9-694d-486b-c711-0d7b98d37360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y, updated_state = model.apply(updated_variables, 1.0, mutable=['counter'])\n",
        "updated_variables = freeze({'params': init_variables['params'], **updated_state})\n",
        "print('updated variables:\\n', updated_variables)\n",
        "print('output:\\n', y)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updated variables:\n",
            " FrozenDict({'params': {'offset': 3}, 'counter': {'count': DeviceArray(4., dtype=float32)}})\n",
            "output:\n",
            " 7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUSOSKegKIM"
      },
      "source": [
        "Here, we should note that even though this example can look a little dumb, it's not very far away from how works batch normalization in practice: you keep a running average of stats related to your data."
      ]
    }
  ]
}