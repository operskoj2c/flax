{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX for the impatient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwtfSYdoHsc_"
      },
      "source": [
        "# JAX for the impatient\n",
        "**JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.**\n",
        "\n",
        "Here we will cover the basics of JAX so that you can get started with Flax, however we very much recommend that you go through JAX's documentation [here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) after going over the basics here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF2oOT78zOIr"
      },
      "source": [
        "##Â NumPy API\n",
        "Let's start by exploring the NumPy API coming from JAX and the main differences you should be aware of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5csM8DZYEqk6"
      },
      "source": [
        "import jax\n",
        "from jax import numpy as jnp, random\n",
        "\n",
        "import numpy as np # We import the standard NumPy library "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5BLL6v_JUSI"
      },
      "source": [
        "`jax.numpy` is the NumPy-like API that needs to be imported, and we will also use `jax.random` to generate some data to work on. \n",
        "\n",
        "Let's start by generating some matrices, and then try matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2HKiLTNJ4Eh",
        "outputId": "c4297a1a-4e4b-4bdc-ca5d-3d33aca92b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "m = jnp.ones((4,4)) # We're generating one 4 by 4 matrix filled with ones.\n",
        "n = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
        "               [5.0, 6.0, 7.0, 8.0]]) # An explicit 2 by 4 array\n",
        "m"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[1., 1., 1., 1.],\n",
              "             [1., 1., 1., 1.],\n",
              "             [1., 1., 1., 1.],\n",
              "             [1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKFtn4d_Nu07"
      },
      "source": [
        "Arrays in JAX are represented as DeviceArray instances and are agnostic to the place where the array lives (CPU, GPU, or TPU). This is why we're getting the warning that no GPU/TPU was found and JAX is falling back to CPU (unless you're running it in an environment that has a GPU/TPU available).\n",
        "\n",
        "We can obviously multiply matrices like we would do in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9do-ZRGaRThn",
        "outputId": "9c4feb4d-3bd1-4921-97ce-c8087b37496f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jnp.dot(n, m).block_until_ready() # Note: yields the same result as n.dot(m)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[10., 10., 10., 10.],\n",
              "             [26., 26., 26., 26.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkyt5xXpRidn"
      },
      "source": [
        "DeviceArray instances are actually futures ([more here](https://jax.readthedocs.io/en/latest/async_dispatch.html)) due to the **default asynchronous execution** in JAX. For that reason, the Python call might return before the computation actually ends, hence we're using the `block_until_ready()` method to ensure we return the end result.\n",
        "\n",
        "JAX is fully compatible with NumPy, and can transparently process arrays from one library to the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFthGlHoRZ59",
        "outputId": "15892d6a-c06c-4f98-a7d4-ad432bdd1f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.random.normal(size=(4,4)) # Creating one standard NumPy array instance\n",
        "jnp.dot(x,m)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-2.1363673 , -2.1363673 , -2.1363673 , -2.1363673 ],\n",
              "             [ 2.5145242 ,  2.5145242 ,  2.5145242 ,  2.5145242 ],\n",
              "             [-0.41032386, -0.41032386, -0.41032386, -0.41032386],\n",
              "             [ 0.29587948,  0.29587948,  0.29587948,  0.29587948]],            dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoaA-FS2XpsC"
      },
      "source": [
        "If you're using accelerators, using NumPy arrays directly will result in multiple transfers from CPU to GPU/TPU memory. You can save that transfer bandwidth, either by creating directly a DeviceArray or by using `jax.device_put` on the NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VABtdIwTFfN",
        "outputId": "08965869-bdd7-44c8-ae46-207061b5112c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.random.normal(size=(4,4))\n",
        "x = jax.device_put(x)\n",
        "x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-2.6163752 ,  0.44878265,  1.5077958 ,  1.4197438 ],\n",
              "             [-0.8818235 ,  0.89041513,  0.8823177 , -0.50059307],\n",
              "             [ 0.8309847 , -0.6925419 ,  0.2270658 ,  1.0040795 ],\n",
              "             [ 1.1299754 , -0.09446267,  0.4083803 ,  0.8279651 ]],            dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_2QavY1tR8j"
      },
      "source": [
        "Conversely, if you want to get back a Numpy array from a JAX array, you can simply do so by using it in the Numpy API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEJ1mSvStjEC",
        "outputId": "00a8cc38-59a2-4cf9-ed23-eb5fbb708495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
        "               [5.0, 6.0, 7.0, 8.0]])\n",
        "np.array(x)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3., 4.],\n",
              "       [5., 6., 7., 8.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBHVd3GTpLKD"
      },
      "source": [
        "## (Im)mutability in JAX\n",
        "JAX is functional by essence, one practical consequence being that JAX arrays are immutable. This means no in-place ops and sliced assignments. More generally, functions should not take input or produce output using a global state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-erZrgZXawFW",
        "outputId": "c3c03081-6235-482f-a88c-cc180f661954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
        "               [5.0, 6.0, 7.0, 8.0]])\n",
        "jax.ops.index_update(x,(0,0), 3.0) # whereas x[0,0] = 3.0 would fail"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[3., 2., 3., 4.],\n",
              "             [5., 6., 7., 8.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkyHWojBsOLE"
      },
      "source": [
        "Index operators can be found in `jax.ops`and follow the `index_*` pattern. To generate index in those syntax, one can use the `jax.ops.index` syntactic sugar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inv0IawpnHIb",
        "outputId": "e9b2d23e-815e-4857-e812-de2f8319f64a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
        "               [5.0, 6.0, 7.0, 8.0]])\n",
        "jax.ops.index_update(x,jax.ops.index[0,:], 3.0) #Â Same as x[O,:] = 3.0 in NumPy."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[3., 3., 3., 3.],\n",
              "             [5., 6., 7., 8.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QGmV0B4TOWA"
      },
      "source": [
        "Finally, a more concise (and modern) way would be to use the `.at` attribute that plays the same syntactic sugar role as previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUUfYRiS7OU",
        "outputId": "398bb225-b551-4e59-f1ac-947a16b6475e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x.at()\n",
        "x.at[0,:].set(3.0) # Note: this returns a new array and doesn't mutate in place."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[3., 3., 3., 3.],\n",
              "             [5., 6., 7., 8.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz_9b-XUTjjl"
      },
      "source": [
        "All jax ops are available with this syntax, including: `set`, `add`, `mul`, `min`, `max`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8QGdusyzbmP"
      },
      "source": [
        "## Managing randomness in JAX\n",
        "In JAX, randomness is managed in a very specific way, and you can read more on JAX's docs [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers) (we borrow content from there!). As the JAX team puts it:\n",
        "\n",
        "*JAX implements an explicit PRNG where entropy production and consumption are handled by explicitly passing and iterating PRNG state. JAX uses a modern Threefry counter-based PRNG thatâs splittable. That is, its design allows us to fork the PRNG state into new PRNGs for use with parallel stochastic generation.*\n",
        "\n",
        "In short, you need to explicitely manage the PRNGs (pseudo random number generators) and their states. In JAX's PRNGs, the state is represented as a pair of two unsigned-int32s that is called a key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iz9KGF4s7nN",
        "outputId": "c5bb1581-090b-42ed-cc42-08436154bc14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0, 0], dtype=uint32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y622foIaYjL"
      },
      "source": [
        "If you use this key multiple times, you will not get any randomness in the output. To generate further entries in the sequence, you'll need to split the PRNG and thus generate a new pair of keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOBv5CaB3dMa",
        "outputId": "ac89afdc-a73e-4c31-d005-7e1e6ad551cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"old key\", key)\n",
        "key, subkey = random.split(key)\n",
        "normal_pseudorandom = random.normal(subkey, shape=(1,)) # We explicitely provide the PRNG as an input to the random matrix generator.\n",
        "print(\"    \\---SPLIT --> new key   \", key)\n",
        "print(\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "old key [0 0]\n",
            "    \\---SPLIT --> new key    [4146024105  967050713]\n",
            "             \\--> new subkey [2718843009 1272950319] --> normal [-1.2515389]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgCCZtyQ4EqA"
      },
      "source": [
        "You can also generate multiple subkeys at once if needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3zRojMs4Cce",
        "outputId": "e48e1ed0-4f16-49cb-dc2b-cb51d3ec56b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "key, *subkeys = random.split(key, 4) # TODO Question : why is there no overlapping result with random.split(key) (so only one number generated with the same key)\n",
        "key, subkeys"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3306097435, 3899823266], dtype=uint32),\n",
              " [array([147607341, 367236428], dtype=uint32),\n",
              "  array([2280136339, 1907318301], dtype=uint32),\n",
              "  array([ 781391491, 1939998335], dtype=uint32)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20lC7np5YKDq"
      },
      "source": [
        "You can think about those PRNGs as trees of keys that matches the structure of your models, which is important for reproducibility and soundness of the random behavior that you expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6-1gq1YsgZ"
      },
      "source": [
        "## Gradients and autodiff\n",
        "\n",
        "For a full overview, you can have a look at JAX's documentation on Autodiff [here](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
        "\n",
        "**TODO: add something about the computational cost vjp vs jvp.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUFwVnn4011l"
      },
      "source": [
        "### Gradients\n",
        "\n",
        "JAX provides first-class support for gradients and automatic differentiation in functions. This is also where the functional paradigm shines, since gradients on functions are essentially stateless operations. If we consider a simple function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$\n",
        "\n",
        "$$f(x) = \\frac{1}{2} x^T x$$\n",
        "with the (known) gradient:\n",
        "$$\\nabla f(x) = x$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDOydrLMcIzp",
        "outputId": "580c14ed-d1a3-4f92-c9b9-78d58c87bc76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "def f(x):\n",
        "  return jnp.dot(x.T,x)/2.0\n",
        "\n",
        "v = random.normal(key,(4,))\n",
        "f(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(2.1347964, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVaiZplShoBK"
      },
      "source": [
        "JAX computes the gradient as an operator acting on functions with `jax.grad`. Note that this only works for scalar valued functions.\n",
        "\n",
        "Let's take the gradient of f and make sure it matches the identity map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ael3pVHmhhTs",
        "outputId": "4d0c5122-1ead-4a94-9153-7eb3b399dae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "v = random.normal(key,(4,))\n",
        "print(\"Original v:\")\n",
        "print(v)\n",
        "print(\"Gradient of f taken at point v\")\n",
        "print(jax.grad(f)(v)) # should be equal to v !"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original v:\n",
            "[ 1.8160858 -0.7548852  0.339889  -0.5348355]\n",
            "Gradient of f taken at point v\n",
            "[ 1.8160858 -0.7548852  0.339889  -0.5348355]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHIMfchIiQMR"
      },
      "source": [
        "As mentionned, previously, `jax.grad` only works for scalar valued functions. JAX can also handle general vector valued functions. The most useful primitives are the so-called jacobian vector product `jax.jvp` and vector jacobian product `jax.vjp`.\n",
        "\n",
        "### Jacobian vector product\n",
        "\n",
        "Let's consider a map $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$. As a reminder, the differential of f is the map $df:\\mathbb{R}^n \\rightarrow \\mathcal{L}(\\mathbb{R}^n,\\mathbb{R}^m)$ where $\\mathcal{L}(\\mathbb{R}^n,\\mathbb{R}^m)$ is the space of linear maps from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ (hence $df(x)$ is often represented as a Jacobian matrix). The linear approximation of f at point $x$ reads:\n",
        "$$f(x+v) = f(x) + df(x)\\bullet v + o(v)$$\n",
        "The $\\bullet$ operator means you are applying the linear map $df(x)$ to the vector v.\n",
        "\n",
        "Even though you are rarely interested in computing the full Jacobian matrix representing the linear map $df(x)$ in a standard basis, you are often interested in the quantity $df(x)\\bullet v$. This is exactly what `jax.jvp` is for, and `jax.jvp(f, (x,), (v,))` returns the tuple:\n",
        "$$(f(x), df(x)\\bullet v)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5nI_gbeqj2y"
      },
      "source": [
        "Let's use a simple function as an example: $f(x) = \\frac{1}{2}({x_1}^2, {x_2}^2, \\ldots, {x_n}^2)$ where we know that $df(x)\\bullet h = (x_1h_1, x_2h_2,\\ldots,x_nh_n)$. Hence using `jax.jvp` with $h= (1,1,\\ldots,1)$ should return $x$ as an output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2ntaHBeh-5u",
        "outputId": "93591ad3-832f-4928-c1f8-073cc3b7aae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def f(x):\n",
        "  return jnp.multiply(x,x)/2.0\n",
        "\n",
        "x = random.normal(key, (5,))\n",
        "v = jnp.ones(5)\n",
        "print(\"(x,f(x))\")\n",
        "print((x,f(x)))\n",
        "print(\"jax.jvp(f, (x,),(v,))\")\n",
        "print(jax.jvp(f, (x,),(v,)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(x,f(x))\n",
            "(DeviceArray([ 0.18784378, -1.2833427 , -0.27109176,  1.2490592 ,\n",
            "              0.24446994], dtype=float32), DeviceArray([0.01764264, 0.82348424, 0.03674537, 0.7800744 , 0.02988278],            dtype=float32))\n",
            "jax.jvp(f, (x,),(v,))\n",
            "(DeviceArray([0.01764264, 0.82348424, 0.03674537, 0.7800744 , 0.02988278],            dtype=float32), DeviceArray([ 0.18784378, -1.2833427 , -0.27109176,  1.2490592 ,\n",
            "              0.24446994], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdm_TTDLal_X"
      },
      "source": [
        "### Vector Jacobian product\n",
        "Keeping our $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ it's often the case (for example when you are working with a scalar loss function) that you are interested in the composition $x\\rightarrow\\phi\\circ f(x)$ where $\\phi \\in \\mathcal{L}(\\mathbb{R}^m,\\mathbb{R})$ (dual space of $\\mathbb{R}^m$). In that case, the gradient reads:\n",
        "$$\\nabla(\\phi\\circ f)(x) = J_f(x)^T\\nabla\\phi(f(x))$$\n",
        "Where $J_f(x)$ is the Jacobian matrix of f evaluated at x, meaning that $df(x)\\bullet v = J_f(x)v$.\n",
        "\n",
        "`jax.vjp(f,x)` returns the tuple:\n",
        "$$(f(x),v\\rightarrow v^TJ_f(x))$$\n",
        "\n",
        "Keeping the same example as previously, using $v=(1,\\ldots,1)$, applying the VJP function returned by JAX should return the $x$ value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1VTl9zXqsFl",
        "outputId": "f3f143a9-b1f1-4a4d-e4b1-c24a0fa114b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(val, jvp_fun) = jax.vjp(f,x)\n",
        "print(\"x = \", x)\n",
        "print(\"v^T Jf(x) = \", jvp_fun(jnp.ones((5,)))[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =  [ 0.18784378 -1.2833427  -0.27109176  1.2490592   0.24446994]\n",
            "v^T Jf(x) =  [ 0.18784378 -1.2833427  -0.27109176  1.2490592   0.24446994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v1Uq_XlzRZS"
      },
      "source": [
        "## Accelerating code with jit & ops vectorization\n",
        "We borrow the following example from the [JAX quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF04t9L71dhH"
      },
      "source": [
        "### Jit\n",
        "\n",
        "JAX uses the XLA compiler under the hood, and enables you to jit compile your code to make it faster and more efficient. This is the purpose of the @jit annotation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6p_wQ9xeIiu",
        "outputId": "af7ea5af-5ee1-4aa5-d8d7-8f6a20da2b0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "v = random.normal(key, (1000000,))\n",
        "%timeit selu(v).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 20.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 3: 6.48 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk9LVX580j6M"
      },
      "source": [
        "Now using the jit annotation (or function here) to speed things up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us5pWySG0jWL",
        "outputId": "e8ff3b7b-3917-40fc-8f29-eb9e6df262e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "selu_jit = jax.jit(selu)\n",
        "%timeit selu_jit(v).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 24.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 1.56 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kQyCgo407oF"
      },
      "source": [
        "jit compilation can be used along with autodiff in the code transparently.\n",
        "\n",
        "---\n",
        "### Vectorization\n",
        "\n",
        "Finally, JAX enables you to write code that apply to a single example, and then vectorize it to manage transparently batching dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-E6MsKF0tmZ",
        "outputId": "bfa377e8-92ee-4473-abd4-8d52338e2cc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mat = random.normal(key, (15, 10))\n",
        "batched_x = random.normal(key, (5, 10)) # Batching on first dimension\n",
        "single = random.normal(key, (10,))\n",
        "\n",
        "def apply_matrix(v):\n",
        "  return jnp.dot(mat, v)\n",
        "\n",
        "print(\"Single apply shape: \", apply_matrix(single).shape)\n",
        "print(\"Batched example shape: \", jax.vmap(apply_matrix)(batched_x).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single apply shape:  (15,)\n",
            "Batched example shape:  (5, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2BcA8wm2_FW"
      },
      "source": [
        "##Â Full example: linear regression\n",
        "\n",
        "Let's implement one of the simplest model using everything we saw so far: linear regression. From a set of data points $\\{(x_i,y_i), i\\in \\{1,\\ldots, k\\}, x_i\\in\\mathbb{R}^n,y_i\\in\\mathbb{R}^m\\}$, we try to find a set of parameters $W\\in \\mathcal{M}_{m,n}(\\mathbb{R}), b\\in\\mathbb{R}^m$ such that the function $f_{W,b}(x)=Wx+b$ minimizes the mean squared error:\n",
        "$$\\mathcal{L}(W,b)\\rightarrow\\frac{1}{k}\\sum_{i=1}^{k} \\frac{1}{2}\\|y_i-f_{W,b}(x_i)\\|^2_2$$\n",
        "(Note: this is a rough explanation, theoretically we should be minimizing the expectation of the loss, however for the sake of simplicity here we consider only the sampled loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W9p_zVe2Cj-"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict(W,b):\n",
        "  def predict(x):\n",
        "    return jnp.dot(W,x)+b\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse(x_batched,y_batched):\n",
        "  def mse(W,b):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      return jnp.inner(y-jnp.dot(W,x)-b,y-jnp.dot(W,x)-b)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkIxjjsduPY"
      },
      "source": [
        "# Set problem dimensions\n",
        "nsamples = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "k1, k2 = random.split(key)\n",
        "W = random.normal(k1, (ydim, xdim))\n",
        "b = random.normal(k2, (ydim,))\n",
        "true_predict = make_predict(W,b)\n",
        "\n",
        "# Generate samples with additional noise\n",
        "ksample, knoise = random.split(k1)\n",
        "x_samples = random.normal(ksample, (nsamples, xdim))\n",
        "y_samples = jax.vmap(true_predict)(x_samples) + 0.1*random.normal(knoise,(nsamples, ydim))\n",
        "\n",
        "# Generate MSE for our samples\n",
        "mse = make_mse(x_samples,y_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L2np6wve_xp",
        "outputId": "9db5c834-d7da-4291-d1ec-d4c39008d5ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Initialize estimated W and b with zeros.\n",
        "What = jnp.zeros_like(W)\n",
        "bhat = jnp.zeros_like(b)\n",
        "\n",
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse(W,b))\n",
        "for i in range(101):\n",
        "  # We perform one gradient update\n",
        "  What, bhat = What - alpha*jax.grad(mse,0)(What,bhat), bhat - alpha*jax.grad(mse,1)(What,bhat)\n",
        "  if (i%5==0):\n",
        "    print(\"Loss step {}: \".format(i), mse(What,bhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss for \"true\" W,b:  0.023639774\n",
            "Loss step 0:  11.096583\n",
            "Loss step 5:  1.1743387\n",
            "Loss step 10:  0.3287935\n",
            "Loss step 15:  0.13981776\n",
            "Loss step 20:  0.073595665\n",
            "Loss step 25:  0.04415299\n",
            "Loss step 30:  0.029408697\n",
            "Loss step 35:  0.021554668\n",
            "Loss step 40:  0.017227937\n",
            "Loss step 45:  0.014798884\n",
            "Loss step 50:  0.013420247\n",
            "Loss step 55:  0.012632713\n",
            "Loss step 60:  0.0121811\n",
            "Loss step 65:  0.011921472\n",
            "Loss step 70:  0.011771994\n",
            "Loss step 75:  0.01168585\n",
            "Loss step 80:  0.011636159\n",
            "Loss step 85:  0.011607497\n",
            "Loss step 90:  0.011590948\n",
            "Loss step 95:  0.011581404\n",
            "Loss step 100:  0.011575884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJGKunxNzrxa"
      },
      "source": [
        "This is obviously an approximate solution to the linear regression problem (solving it would require a bit more work!), but here you have all the tools you would need if you wanted to do it the proper way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQXmL86aUS9x"
      },
      "source": [
        "## Refining a bit with pytrees\n",
        "\n",
        "Here we're going to elaborate on our previous example using JAX pytree datastructure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZMUvyCgUzby"
      },
      "source": [
        "### Pytrees basics\n",
        "\n",
        "The JAX ecosystem uses pytrees everywhere and we do as well in Flax (the previous FrozenDict example is one, we'll get back to this). For a complete overview, we suggest that you take a look at the [pytree page](https://jax.readthedocs.io/en/latest/pytrees.html) from JAX's doc:\n",
        "\n",
        "*In JAX, a pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts (JAX can be extended to consider other container types as pytrees, see Extending pytrees below). A leaf element is anything thatâs not a pytree, e.g. an array. In other words, a pytree is just a possibly-nested standard or user-registered Python container. If nested, note that the container types do not need to match. A single âleafâ, i.e. a non-container object, is also considered a pytree.*\n",
        "\n",
        "```python\n",
        "[1, \"a\", object()] # 3 leaves\n",
        "\n",
        "(1, (2, 3), ()) # 3 leaves\n",
        "\n",
        "[1, {\"k1\": 2, \"k2\": (3, 4)}, 5] # 5 leaves\n",
        "```\n",
        "\n",
        "JAX provides a few utilities to work with pytrees that lives in the `tree_util` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SNY5eA1UdkJ"
      },
      "source": [
        "from jax import tree_util\n",
        "\n",
        "t = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LujWjwVQUeea"
      },
      "source": [
        "You will often come across `tree_map` function that maps a function f to a tree and its leaves. We used it in the previous section to display the shapes of the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szDhssVBUjTa",
        "outputId": "9ae4ebf1-a3c4-4ecb-b3df-67c8450310f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree_util.tree_map(lambda x: x*x, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, {'k1': 4, 'k2': (9, 16)}, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s167WGKUlZ9"
      },
      "source": [
        "A more flexible version of `tree_map` would be `tree_multimap`. Instead of applying a standalone function to each of the tree leafs, you also provide a tuple of additional trees with similar shape to the input tree that will provide per leaf arguments to the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNOYK_E7UnOh",
        "outputId": "d211bf85-5993-488c-9fec-aeaf375df007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree_util.tree_multimap(lambda x,y: x+y, t, tree_util.tree_map(lambda x: x*x, t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, {'k1': 6, 'k2': (12, 20)}, 30]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnE75pvlVDO5"
      },
      "source": [
        "### Linear regression with Pytrees\n",
        "\n",
        "Whereas our previous example was perfectly fine, we can see that when things get more complicated (as they will with neural networks), it will be harder to manage parameters of the models as we did. Here we show an alternative based on pytrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v8gNkvUVZnl"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict_pytree(params):\n",
        "  def predict(x):\n",
        "    return jnp.dot(params['W'],x)+params['b']\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse_pytree(x_batched,y_batched):\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      return jnp.inner(y-jnp.dot(params['W'],x)-params['b'],y-jnp.dot(params['W'],x)-params['b'])/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKP0X8rnWAiA"
      },
      "source": [
        "Our `params` is a pytree containing both the `W` and `b` entries. We now generate our loss using pytree params. The great thing is that JAX is able to handle differentiation with respect to pytree parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zc7cMaiWSny",
        "outputId": "a69605cb-1eed-4f81-fc2e-93646c9694dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate MSE for our samples\n",
        "mse_pytree = make_mse_pytree(x_samples,y_samples)\n",
        "\n",
        "# Initialize estimated W and b with zeros.\n",
        "params = {'W': jnp.zeros_like(W), 'b': jnp.zeros_like(b)}\n",
        "\n",
        "jax.grad(mse_pytree)(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': DeviceArray([[-1.9287349e+00,  4.2963740e-01,  7.1613449e-01,\n",
              "                2.1056123e+00,  5.0405109e-01, -2.4983375e+00,\n",
              "               -6.3854158e-01, -2.2620211e+00, -1.3365203e+00,\n",
              "               -2.0426038e-01],\n",
              "              [ 1.1999468e+00, -9.4563609e-01, -1.0878401e+00,\n",
              "               -7.0340693e-01,  3.3224618e-01,  1.7538788e+00,\n",
              "               -7.1916533e-01,  1.0927426e+00, -1.4491035e+00,\n",
              "                5.9715635e-01],\n",
              "              [-1.4826508e+00, -7.6116526e-01,  2.2319850e-01,\n",
              "               -3.0391920e-01,  3.0397053e+00, -3.8419446e-01,\n",
              "               -1.8290073e+00, -2.3353369e+00, -1.1087126e+00,\n",
              "               -7.7454001e-01],\n",
              "              [ 8.2374448e-01, -9.9650615e-01, -7.6030099e-01,\n",
              "                6.3919234e-01, -6.0864888e-02, -1.0859717e+00,\n",
              "                1.2923399e+00, -4.9342909e-01, -1.4710285e-03,\n",
              "                1.2977618e+00],\n",
              "              [-4.5656443e-01, -1.3063036e-01, -3.9178997e-01,\n",
              "                2.1743820e+00, -5.3948764e-02,  4.5653120e-01,\n",
              "               -8.5279429e-01,  1.1709595e+00,  9.6438831e-01,\n",
              "               -2.3813806e-02]], dtype=float32),\n",
              " 'b': DeviceArray([ 1.0923628,  1.3121076, -2.9304824, -0.6492362,  1.153125 ],            dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1IKnjqXFdN"
      },
      "source": [
        "Now using our tree of params, we can write the gradient descent in a simpler way using `jax.tree_multimap`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEntdcDBXBCj",
        "outputId": "f309aff7-2aad-453f-ad88-019d967d4289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse_pytree({'W':W,'b':b}))\n",
        "for i in range(101):\n",
        "  # We perform one gradient update\n",
        "  params = jax.tree_multimap(lambda old,grad: old-alpha*grad, params, jax.grad(mse_pytree)(params))\n",
        "  if (i%5==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_pytree(params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss for \"true\" W,b:  0.023639774\n",
            "Loss step 0:  11.096583\n",
            "Loss step 5:  1.1743387\n",
            "Loss step 10:  0.3287935\n",
            "Loss step 15:  0.13981776\n",
            "Loss step 20:  0.073595665\n",
            "Loss step 25:  0.04415299\n",
            "Loss step 30:  0.029408697\n",
            "Loss step 35:  0.021554668\n",
            "Loss step 40:  0.017227937\n",
            "Loss step 45:  0.014798884\n",
            "Loss step 50:  0.013420247\n",
            "Loss step 55:  0.012632713\n",
            "Loss step 60:  0.0121811\n",
            "Loss step 65:  0.011921472\n",
            "Loss step 70:  0.011771994\n",
            "Loss step 75:  0.01168585\n",
            "Loss step 80:  0.011636159\n",
            "Loss step 85:  0.011607497\n",
            "Loss step 90:  0.011590948\n",
            "Loss step 95:  0.011581404\n",
            "Loss step 100:  0.011575884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh-oo8jFUPNQ"
      },
      "source": [
        "That's all you needed to know to get started with Flax! To dive deeper, we very much recommend to check JAX [docs](https://jax.readthedocs.io/en/latest/index.html)."
      ]
    }
  ]
}