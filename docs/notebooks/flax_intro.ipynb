{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flax intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      },
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ubnMn66ga4Z"
      },
      "source": [
        "# Flax introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp, random, jit, lax\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "import flax\n",
        "from flax import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HMNMi98ejnll"
      },
      "source": [
        "## Intro to Jax\n",
        "\n",
        "Jax is a numerical computation library which aims to replicate the numpy api.\n",
        "\n",
        "A few important things to know about Jax:\n",
        "1. It is functional. This means no in-place ops and sliced assignments\n",
        "2. Jax can execute computaitons on CPUs, GPUs, and TPUs.\n",
        "3. functions using the jax.numpy api can be traced for automatic transformations\n",
        "\n",
        "    a. **jit**: compile a function using XLA enabling fast execution\n",
        "\n",
        "    b. **grad**: take the gradient of a function\n",
        "\n",
        "    c. **vmap**: adds a batch dimension to a function\n",
        "\n",
        "    d. **pmap**: split a computation across devices based on the first dimension of each input argument.\n",
        "\n",
        "\n",
        "## Neural Networks in Jax\n",
        "\n",
        "Before we dive into Flax what a typical neural networks component looks like when written in \"native\" Jax.\n",
        "\n",
        "We decompose a learnable linear layer into two parts: a initializer function which uses a jax PRNGKey to generate a random kernel and bias and the apply function which computes the linear transformation using a set of parameters and some inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dense_init(rng, in_features, out_features,\n",
        "               kernel_init=jax.nn.initializers.lecun_normal(),\n",
        "               bias_init=jax.nn.initializers.zeros):\n",
        "  k1, k2 = random.split(rng)\n",
        "  # init functions take a PRNGKey and a shape tuple and return ndarrays.\n",
        "  kernel = kernel_init(k1, (in_features, out_features))\n",
        "  bias = bias_init(k2, (out_features,))\n",
        "  return kernel, bias\n",
        "\n",
        "def dense_apply(params, inputs):\n",
        "  kernel, bias = params\n",
        "  return jnp.dot(inputs, kernel) + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hwwff8vUJ6t7"
      },
      "source": [
        "Functional Programming without abstractions naturally results into somewhat verbose but very explicit code.\n",
        "\n",
        "Note how the random number generators and parameters are passed on explicitly to functions.\n",
        "Jax has no concept of variables so we cannot hide the parameters in variables somewhere.\n",
        "Similairly, there is no global random number generator which updates an internal seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(DeviceArray([[ 0.56129605,  0.98355806, -1.2807567 , -1.0105268 ],\n             [ 0.05945718, -0.10787111, -0.6869378 , -0.09312173]],            dtype=float32), DeviceArray([0., 0., 0., 0.], dtype=float32))\n"
        }
      ],
      "source": [
        "params = dense_init(random.PRNGKey(0), in_features=2, out_features=4)\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uiEnIech21gc"
      },
      "source": [
        "Once we generated a set of parameters it is easy enough to apply them to some inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "DeviceArray([[ 0.6207532 ,  0.87568694, -1.9676945 , -1.1036485 ]], dtype=float32)"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = jnp.ones((1, 2))\n",
        "dense_apply(params, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VPKvLaZl3BP1"
      },
      "source": [
        "Because everything is functional we can use the functional transformations that jax provides to do useful things like taking gradients to optimize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "(DeviceArray([[ 0.3103766 ,  0.43784347, -0.98384726, -0.5518243 ],\n              [ 0.3103766 ,  0.43784347, -0.98384726, -0.5518243 ]],            dtype=float32),\n DeviceArray([ 0.3103766 ,  0.43784347, -0.98384726, -0.5518243 ], dtype=float32))"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def loss_fn(params, x):\n",
        "  y = dense_apply(params, x)\n",
        "  return jnp.mean(y ** 2)\n",
        "grad_fn = jax.grad(loss_fn) # by default jax.grad takes the gradient w.r.t. the first argument\n",
        "grad_fn(params, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__CrzjEfqE5L"
      },
      "source": [
        "## Flax Modules\n",
        "\n",
        "The core of Flax is the Module abstraction.\n",
        "Modules allow you to write parameterized functions just as if you were writing a normal numpy function with Jax.\n",
        "The module api allows you to declare parameters and use them directly with the Jax api's.\n",
        "\n",
        "A few things to know about Modules:\n",
        "1. A Module is creates by defining a subclass of `flax.nn.Module` and implementing the apply method.\n",
        "2. parameters are declared using `self.param(name, shape, init_func)` and return an initialized parameter value.\n",
        "3. `Dense.init(rng, ...)` and `Dense.call(params, ...)` behave identically to the `dense_init` and `dense_apply` implemented earlier.\n",
        "\n",
        "Now let's try to do redefine the dense layer using Flax Modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dense(nn.Module):\n",
        "  \"\"\"A learned linear transformation.\"\"\"\n",
        "  def apply(self, x, features,\n",
        "            kernel_init=jax.nn.initializers.lecun_normal(),  # init functions are of the form (PrngKey, shape) => init_value\n",
        "            bias_init=jax.nn.initializers.zeros):\n",
        "    in_features = x.shape[-1]\n",
        "    kernel_shape = (in_features, features)\n",
        "    kernel = self.param('kernel', kernel_shape, kernel_init)\n",
        "    bias = self.param('bias', (features,), bias_init)\n",
        "    return jnp.dot(x, kernel) + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'kernel': DeviceArray([[ 0.04612674, -0.41086802,  0.29959238,  0.65631014],\n             [-0.8772928 ,  0.31151664, -0.06712601,  0.7201758 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}\n"
        }
      ],
      "source": [
        "y, params = Dense.init(random.PRNGKey(0), x, features=4)\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "DeviceArray([[-0.8311661 , -0.09935138,  0.23246637,  1.376486  ]], dtype=float32)"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Dense.call(params, x, features=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ednxR882N4kX"
      },
      "source": [
        "Note how we must specify the number of features in both init and call.\n",
        "Modules will often have certain parameters that are fixed for each call to init and call.\n",
        "\n",
        "We can use `Module.partial` to apply these arguments. partial takes keyword arguments and returns a new module for which the given arguments are already applied.\n",
        "It can be thought of as the equivalant of `functools.partial` for Modules. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "DeviceArray([[-0.8311661 , -0.09935138,  0.23246637,  1.376486  ]], dtype=float32)"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_def = Dense.partial(features=4) # Module + hyper parameters = model defintion\n",
        "_, params = model_def.init(random.PRNGKey(0), x)\n",
        "model_def.call(params, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Q1oe0WhOkxC"
      },
      "source": [
        "### Composition\n",
        "\n",
        "Modules can be composed to form more complex Modules.\n",
        "\n",
        "Within a Module's apply function other modules behave just like functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0.5629978  0.51556504 0.05943576 0.68959564]]\n"
        }
      ],
      "source": [
        "# same as flax.nn.relu\n",
        "def relu(x):\n",
        "  return jnp.maximum(0., x)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi Layer Perceptron.\"\"\"\n",
        "  \n",
        "  def apply(self, x,\n",
        "            hidden_features,\n",
        "            output_features,\n",
        "            activation_fn):\n",
        "\n",
        "    z = Dense(x, hidden_features)\n",
        "    h = activation_fn(z)\n",
        "    y = Dense(h, output_features)\n",
        "    return y\n",
        "\n",
        "model_def = MLP.partial(hidden_features=8, output_features=4, activation_fn=relu)\n",
        "y, params = model_def.init(random.PRNGKey(0), x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o6CPJofni4vf"
      },
      "source": [
        "`jax.tree_map` allows you to apply a function to each leave of a pytree.\n",
        "A pytree can consist of (nested) lists, tuples, dicts and other types that contain arrays.\n",
        "\n",
        "We use the tree_map util to reveal the parameter structure of the MLP model\n",
        "\n",
        "Here we can see that composing modules results in a structure of nested dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "{'0': {'bias': (8,), 'kernel': (2, 8)}, '1': {'bias': (4,), 'kernel': (8, 4)}}"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jax.tree_map(np.shape, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGyMhq1rmBRr"
      },
      "source": [
        "#### Module name\n",
        "By default Flax will use integers as keys for the parameters of sub modules. By passing the name argument we can control the parameter structure and make it more meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "{'hidden': {'bias': (8,), 'kernel': (2, 8)},\n 'out': {'bias': (4,), 'kernel': (8, 4)}}"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class NamedMLP(nn.Module):\n",
        "  def apply(self, x,\n",
        "            hidden_features,\n",
        "            output_features,\n",
        "            activation_fn):\n",
        "\n",
        "    z = Dense(x, hidden_features, name='hidden')\n",
        "    h = activation_fn(z)\n",
        "    y = Dense(h, output_features, name='out')\n",
        "    return y\n",
        "\n",
        "model_def = NamedMLP.partial(hidden_features=8, output_features=4, activation_fn=relu)\n",
        "_, params = model_def.init(random.PRNGKey(0), x)\n",
        "jax.tree_map(np.shape, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_L5AOQ97mZAA"
      },
      "source": [
        "### Parameter sharing\n",
        "\n",
        "Sometimes a module should be applied to multiple inputs with one set of parameters.\n",
        "We can make a Module for which parameters are shared between calls using `Module.shared`.\n",
        "Just like with `Module.partial` we can pass keyword arguments that are fixed for each call to the module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "  def apply(self, x, iterations=3):\n",
        "    dense = Dense.shared(\n",
        "        features=x.shape[-1],\n",
        "        kernel_init=jax.nn.initializers.orthogonal(),\n",
        "        name='cell')\n",
        "    ys = []\n",
        "    for i in range(iterations):\n",
        "      x = dense(x)\n",
        "      ys.append(x)\n",
        "    return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bn396SL6RARD"
      },
      "source": [
        "we call the Dense layer named 'cell' 3 times but only one set of parameters shows up in the parameter structure due to weight sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[DeviceArray([[-0.8121684,  1.1577489]], dtype=float32), DeviceArray([[-1.2806695, -0.5999045]], dtype=float32), DeviceArray([[ 0.36959398, -1.3650641 ]], dtype=float32)]\n"
        },
        {
          "data": {
            "text/plain": "{'cell': {'bias': (2,), 'kernel': (2, 2)}}"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ys, params = SimpleRNN.init(random.PRNGKey(0), x)\n",
        "print(ys)\n",
        "jax.tree_map(np.shape, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lFUvB2IcZJpu"
      },
      "source": [
        "### Shape inference\n",
        "\n",
        "Previously we initialized the model by passing in some inputs.\n",
        "This is useful because it allows for Modules which automatically infer the shape of parameters based on inputs. It can also help catch errors in the model early, in the initialization phase of a program.\n",
        "\n",
        "Nontheless, `Module.init` includes some unnecesary overhead because typically we are not interested in the actual output of the model during initialization. Therefore, we can use Jax build in lazy evaluation to get the benefits of shape inference without doing any unnecesary compute.\n",
        "\n",
        "`Module.init_by_shape` returns only the shape and dtype of outputs but still creates fully initialized parameters. If you want to use initializers that (indirectly) depend on the values (not shape) of the inputs you should keep using `Module.init`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[DeviceArray([[-0.8121684,  1.1577489]], dtype=float32), DeviceArray([[-1.2806695, -0.5999045]], dtype=float32), DeviceArray([[ 0.36959398, -1.3650641 ]], dtype=float32)]\n"
        },
        {
          "data": {
            "text/plain": "{'cell': {'bias': (2,), 'kernel': (2, 2)}}"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_spec = [((1, 2), jnp.float32)]\n",
        "out_spec, params = SimpleRNN.init_by_shape(random.PRNGKey(0), input_spec)\n",
        "print(ys)\n",
        "jax.tree_map(np.shape, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ShapeDtypeStruct(shape=(1, 2), dtype=float32), ShapeDtypeStruct(shape=(1, 2), dtype=float32), ShapeDtypeStruct(shape=(1, 2), dtype=float32)]\n[(1, 2), (1, 2), (1, 2)]\n"
        }
      ],
      "source": [
        "print(out_spec) # the outputs only define the shape and dtype\n",
        "print([spec.shape for spec in out_spec])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EAdFRvBwRran"
      },
      "source": [
        "### Model\n",
        "\n",
        "Module makes it easy to keep track of parameters inside a Model but so far it still required explicilty keeping track of parameter structure and the init & apply functions.\n",
        "\n",
        "Model is a thin abstraction around a Module and a set of parameter.\n",
        "A Model instance is callable and functional (eg. changing parameters requires a new model instance).\n",
        "\n",
        "Using `Module.create` or `Module.create_by_shape` will create a newly initialized set of parameters and wrap them in a Model instance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JcWnAjqqSbXj"
      },
      "outputs": [],
      "source": [
        "x = jnp.ones((1, 2))\n",
        "ys, model = Dense.partial(features=4).create(random.PRNGKey(0), x)\n",
        "jax.tree_map(np.shape, model.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XntK4ZKcPDrc"
      },
      "outputs": [],
      "source": [
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "udBSzSoz_ipZ"
      },
      "outputs": [],
      "source": [
        "model.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDvnJ1VP_qfz"
      },
      "source": [
        "Parameters can be updated using the `Model.replace` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mDI0Ovi6_lWS"
      },
      "outputs": [],
      "source": [
        "biased_model = model.replace(params={'kernel': model.params['kernel'], 'bias': model.params['bias'] + 1.})\n",
        "biased_model.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8PAwSzbA29M"
      },
      "source": [
        "Model is registerd as a container object which means that it can be passed to Jax transformations and `jax.tree_map`.\n",
        "\n",
        "For example we can take gradients w.r.t. a model object. The returned Model object will contain the gradients corresponding to each parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PXacZ7chAU5O"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model):\n",
        "  y = model(x)\n",
        "  return jnp.mean(y ** 2)\n",
        "\n",
        "model_grad = jax.grad(loss_fn)(model)\n",
        "model_grad.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yHxj5gVhAzLX"
      },
      "source": [
        "### State\n",
        "\n",
        "Flax allows stateful operations to happen within a limited scope.\n",
        "\n",
        "stateful modules are defined using the `Module.state` api. It returns a state object that has a property value that can be assigned to.\n",
        "\n",
        "A typical use stateful module is BatchNorm which maintains a moving average of batch statistics (mean, variance).\n",
        "During training the moving averages are updated such that they can be used during test time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "85xRFNuYAy2f"
      },
      "outputs": [],
      "source": [
        "# simplified version of nn.BatchNorm\n",
        "class BatchNorm(nn.Module):\n",
        "  def apply(self, x, red_axis=0, eps=1e-5,\n",
        "            momentum=0.99, training=False,\n",
        "            gamma_init=nn.initializers.ones,\n",
        "            beta_init=nn.initializers.zeros):\n",
        "\n",
        "    # compute the moments of the input\n",
        "    mean = x.mean(red_axis, keepdims=True)\n",
        "    var = jnp.square(x - mean).mean(red_axis, keepdims=True)\n",
        "\n",
        "    # define the state variables\n",
        "    ra_mean = self.state('mean', mean.shape, nn.initializers.zeros)\n",
        "    ra_var = self.state('var', var.shape, nn.initializers.ones)\n",
        "\n",
        "    if not self.is_initializing():  # during init we ignore the moving averages completly\n",
        "      if training:\n",
        "        # during training the moving averages are updated\n",
        "        alpha = 1. - momentum\n",
        "        ra_mean.value += alpha * (mean - ra_mean.value)\n",
        "        ra_var.value += alpha * (var - ra_var.value)\n",
        "      else:\n",
        "        # if we are not training we use the moving averages\n",
        "        mean = ra_mean.value\n",
        "        var = ra_var.value\n",
        "\n",
        "    # standardize the input\n",
        "    y = (x - mean) / jnp.sqrt(var + eps)\n",
        "\n",
        "    # learn the scale and bias of the output\n",
        "    gamma = self.param('gamma', mean.shape, gamma_init)\n",
        "    beta = self.param('beta', mean.shape, beta_init)\n",
        "    return gamma * y + beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QKoNxlq84lHk"
      },
      "source": [
        "Stateful modules require special care when used. The `nn.stateful` context manager defines a scope in which stateful operations are allowed. Outside of this scope the state becomes immutable.\n",
        "\n",
        "The state is stored in a `nn.Collection` object which internally stores the state as a dictionary.\n",
        "\n",
        "`nn.stateful` takes a Collection containing the current state and returns a new Collection that contains the updated state. By default a new Collection will be created.\n",
        "\n",
        "When using `nn.stateful(state, mutable=False)` the state can be read but any updates will raise an error. This is often useful during test time to garantuee test data does not affect the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NhL1BGCGkmsp"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "  def apply(self, x, training=False):\n",
        "    x = Dense(x, features=4)\n",
        "    x = BatchNorm(x, training=training, momentum=0., name='batch_norm')\n",
        "    return x\n",
        "\n",
        "dist_a = lambda rng, shape: random.normal(rng, shape) * jnp.array([[1., 3.]])\n",
        "\n",
        "x_a = dist_a(random.PRNGKey(1), (1024, 2))\n",
        "print('std. deviation of input:', x_a.std(0))\n",
        "\n",
        "with nn.stateful() as init_state:\n",
        "  y, params = MyModel.init(random.PRNGKey(2), x_a)\n",
        "print('std. deviation of output (init):', y.std(0))\n",
        "\n",
        "with nn.stateful(init_state) as new_state:\n",
        "  y = MyModel.call(params, x_a, training=True)\n",
        "print('std. deviation of output (training):', y.std(0))\n",
        "\n",
        "with nn.stateful(new_state, mutable=False):\n",
        "  y = MyModel.call(params, x_a, training=False)\n",
        "print('std. deviation of output (testing):', y.std(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M2DJaEVV6I67"
      },
      "source": [
        "The state can be inspected using `Collection.as_dict()`.\n",
        "\n",
        "Each Module has a path like key into the Collection (eg. '/some_module/nested_module/dense')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fWFLKYjPmMus"
      },
      "outputs": [],
      "source": [
        "init_state.as_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zQ5YXCD-mRZC"
      },
      "outputs": [],
      "source": [
        "new_state.as_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NWqk1SBZ6iWD"
      },
      "source": [
        "The stateful mechanism forces the user to be explicit about stateful operations.\n",
        "\n",
        "One motivating example for this approach is to enforce that state is not updated at test time.\n",
        "\n",
        "Another benefit is that it is easier to replace the state when necessary.\n",
        "For example let say we want to apply this model on a second input distribution (b) with different statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H_M0Xc2uk7bX"
      },
      "outputs": [],
      "source": [
        "dist_b = lambda rng, shape: random.normal(rng, shape) * jnp.array([[2., 5.]])\n",
        "\n",
        "x_b = dist_b(random.PRNGKey(1), (1024, 2))\n",
        "\n",
        "with nn.stateful(new_state, mutable=False):\n",
        "  y = MyModel.call(params, x_b, training=False)\n",
        "print(y.std(0)) # this will not be properly normalized!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nWdH5jcr7d05"
      },
      "source": [
        "We can solve the skew in statistics by creating a seperate state for this alternative input distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fTvDFgm2lJb_"
      },
      "outputs": [],
      "source": [
        "with nn.stateful(init_state) as state_b:\n",
        "  y = MyModel.call(params, x_b, training=True)\n",
        "print('std. deviation of output (training):', y.std(0))\n",
        "\n",
        "with nn.stateful(state_b, mutable=False):\n",
        "  y = MyModel.call(params, x_b, training=False)\n",
        "print('std. deviation of output (testing):', y.std(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KEfIkD3uR8hh"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jxX4_bBbPJLx"
      },
      "outputs": [],
      "source": [
        "rng = random.PRNGKey(0)\n",
        "rng, key1, key2 = random.split(rng, 3)\n",
        "n = 30\n",
        "x = jnp.linspace(-5., 5.)\n",
        "X = random.uniform(key1, (n,), minval=-5., maxval=5.)\n",
        "f = lambda x: 2. * x\n",
        "Y = f(X) + random.normal(key2, (n,))\n",
        "plt.plot(x, f(x))\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "45OyrTLJQmbh"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def apply(self, x):\n",
        "    return nn.Dense(x[..., None], features=1)[..., 0]\n",
        "\n",
        "rng, key = random.split(rng)\n",
        "_, model = LinearRegression.create(key, X)\n",
        "\n",
        "plt.plot(x, f(x))\n",
        "plt.plot(x, model(x))\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3gpmflp9Rn0M"
      },
      "outputs": [],
      "source": [
        "optimizer_def = optim.Momentum(learning_rate=0.1)\n",
        "optimizer = optimizer_def.create(model)\n",
        "\n",
        "train_steps = 100\n",
        "\n",
        "def loss_fn(model):\n",
        "  Y_hat = model(X)\n",
        "  return jnp.square(Y - Y_hat).mean()\n",
        "\n",
        "for i in range(train_steps):\n",
        "  optimizer, loss = optimizer.optimize(loss_fn)\n",
        "print('mean square error:', loss)\n",
        "\n",
        "trained_model = optimizer.target\n",
        "print(trained_model.params)\n",
        "plt.plot(x, f(x))\n",
        "plt.plot(x, trained_model(x))\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jlUWZUrNovE8"
      },
      "outputs": [],
      "source": [
        "loss, grads = optimizer.compute_gradients(loss_fn)\n",
        "new_optimizer = optimizer.apply_gradient(grads)\n",
        "new_optimizer.target.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1KCPh73QSsLr"
      },
      "source": [
        "## Advanced features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJyqXS1RSvXg"
      },
      "source": [
        "### Selective Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wltd0AA2ue9L"
      },
      "outputs": [],
      "source": [
        "slope_opt_def = optim.Momentum(learning_rate=0.1)\n",
        "bias_opt_def = optim.Momentum(learning_rate=0.1, weight_decay=10.)\n",
        "slope_traversal = optim.ModelParamTraversal(lambda name, param: 'kernel' in name)\n",
        "bias_traversal = optim.ModelParamTraversal(lambda name, param: 'bias' in name)\n",
        "optimizer_def = optim.MultiOptimizer((slope_traversal, slope_opt_def), (bias_traversal, bias_opt_def))\n",
        "\n",
        "optimizer = optimizer_def.create(model)\n",
        "\n",
        "train_steps = 100\n",
        "\n",
        "def loss_fn(model):\n",
        "  Y_hat = model(X)\n",
        "  return jnp.square(Y - Y_hat).mean()\n",
        "\n",
        "for i in range(train_steps):\n",
        "  optimizer, loss = optimizer.optimize(loss_fn)\n",
        "print('mean square error:', loss)\n",
        "\n",
        "trained_model = optimizer.target\n",
        "print(trained_model.params)\n",
        "plt.plot(x, f(x))\n",
        "plt.plot(x, trained_model(x))\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CHQhNahuS0ed"
      },
      "source": [
        "### Multi method modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mNhjLdIWFHLn"
      },
      "outputs": [],
      "source": [
        "class MultiMethodModule(nn.Module):\n",
        "\n",
        "  def apply(self, x):\n",
        "    kernel = self.param('kernel', (), lambda _, shape: jnp.full(shape, 2.))\n",
        "    return x * kernel\n",
        "\n",
        "  @nn.module_method\n",
        "  def decode(self, x):\n",
        "    kernel = self.get_param('kernel')\n",
        "    return x * kernel\n",
        "\n",
        "x = 2. ** jnp.arange(5)\n",
        "y, model = MultiMethodModule.create(random.PRNGKey(0), x)\n",
        "print(x[1:], y[:-1])\n",
        "model.decode(1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ElRsCnAUHARX"
      },
      "outputs": [],
      "source": [
        "def body_fn(x, _):\n",
        "  y = model.decode(x)\n",
        "  return y, y\n",
        "\n",
        "lax.scan(body_fn, 1., (), length=4)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zbA5RyHVTC_V"
      },
      "source": [
        "### Transforming sub module parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SoTV_vCbHERM"
      },
      "outputs": [],
      "source": [
        "def add_scale(module):\n",
        "  class ScaleWrapper(nn.Module):\n",
        "    \"\"\"Add a learnable scale to the kernel of a module.\"\"\"\n",
        "\n",
        "    def apply(self, *args, **kwargs):\n",
        "      def init_fn(rng, _):\n",
        "        _, params = module.init(rng, *args, **kwargs)\n",
        "        # here we could change the initial parameters of the wrapped module\n",
        "        return params\n",
        "      params = self.param('params', None, init_fn)\n",
        "      # here change transform parameters every call\n",
        "      assert 'kernel' in params\n",
        "      kernel = params['kernel']\n",
        "      features = kernel.shape[-1]\n",
        "      scale = self.param('scale', (features,), nn.initializers.ones)\n",
        "      scaled_kernel = kernel * scale\n",
        "      scaled_params = params.copy()\n",
        "      scaled_params['kernel'] = scaled_kernel\n",
        "\n",
        "      return module.call(scaled_params, *args, **kwargs)\n",
        "  return ScaleWrapper\n",
        "\n",
        "x = jnp.ones((1, 2))\n",
        "model_def = add_scale(Dense).partial(features=4)\n",
        "y, params = model_def.init(random.PRNGKey(0), x)\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qkVHN3Wmr8"
      },
      "source": [
        "### Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hcY47fUFTLqQ"
      },
      "source": [
        "## Gotchas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yeS1BSYRvO4u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}