diff --git a/examples/mnist/train.py b/examples/mnist/train.py
index 51d2fde..0ca3365 100644
--- a/examples/mnist/train.py
+++ b/examples/mnist/train.py
@@ -23,6 +23,9 @@ from absl import app
 from absl import flags
 from absl import logging
 
+import functools
+
+from flax import jax_utils
 from flax import nn
 from flax import optim
 
@@ -80,12 +83,14 @@ class CNN(nn.Module):
     return x
 
 
+@jax.pmap
 def create_model(key):
   _, initial_params = CNN.init_by_shape(key, [((1, 28, 28, 1), jnp.float32)])
   model = nn.Model(CNN, initial_params)
   return model
 
 
+@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2))
 def create_optimizer(model, learning_rate, beta):
   optimizer_def = optim.Momentum(learning_rate=learning_rate, beta=beta)
   optimizer = optimizer_def.create(model)
@@ -111,7 +116,7 @@ def compute_metrics(logits, labels):
   return metrics
 
 
-@jax.jit
+@jax.pmap
 def train_step(optimizer, batch):
   """Train for a single step."""
   def loss_fn(model):
@@ -125,7 +130,7 @@ def train_step(optimizer, batch):
   return optimizer, metrics
 
 
-@jax.jit
+@jax.pmap
 def eval_step(model, batch):
   logits = model(batch['image'])
   return compute_metrics(logits, batch['label'])
@@ -142,16 +147,23 @@ def train_epoch(optimizer, train_ds, batch_size, epoch, rng):
   batch_metrics = []
   for perm in perms:
     batch = {k: v[perm] for k, v in train_ds.items()}
+    batch = jax_utils.replicate(batch)
     optimizer, metrics = train_step(optimizer, batch)
     batch_metrics.append(metrics)
 
   # compute mean of metrics across each batch in epoch.
   batch_metrics_np = jax.device_get(batch_metrics)
+  # stack all of the metrics from each devioce into
+  # numpy arrays directly on a dict, such that, e.g.
+  # `batch_metrics_np['loss']` has shape (jax.device_count(), )
+  batch_metrics_np = jax.tree_multimap(lambda *xs: onp.array(xs),
+                                       *batch_metrics_np)
   epoch_metrics_np = {
-      k: onp.mean([metrics[k] for metrics in batch_metrics_np])
-      for k in batch_metrics_np[0]}
-
-  logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,
+      k: onp.mean(batch_metrics_np[k], axis=0)
+      for k in batch_metrics_np
+  }
+  # `epoch_metrics_np` now contains 1D arrays of length `jax.device_count()`
+  logging.info('train epoch: %d, loss: %s, accuracy: %s', epoch,
                epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100)
 
   return optimizer, epoch_metrics_np
@@ -160,7 +172,7 @@ def train_epoch(optimizer, train_ds, batch_size, epoch, rng):
 def eval_model(model, test_ds):
   metrics = eval_step(model, test_ds)
   metrics = jax.device_get(metrics)
-  summary = jax.tree_map(lambda x: x.item(), metrics)
+  summary = metrics
   return summary['loss'], summary['accuracy']
 
 
@@ -178,16 +190,19 @@ def train(train_ds, test_ds):
   batch_size = FLAGS.batch_size
   num_epochs = FLAGS.num_epochs
 
+  rng = random.split(rng, jax.device_count())
   model = create_model(rng)
   optimizer = create_optimizer(model, FLAGS.learning_rate, FLAGS.momentum)
 
   input_rng = onp.random.RandomState(0)
+  test_ds = jax_utils.replicate(test_ds)
 
   for epoch in range(1, num_epochs + 1):
     optimizer, _ = train_epoch(
         optimizer, train_ds, batch_size, epoch, input_rng)
     loss, accuracy = eval_model(optimizer.target, test_ds)
-    logging.info('eval epoch: %d, loss: %.4f, accuracy: %.2f',
+    # `loss` and `accuracy` are now 1D arrays of length `jax.device_count()`
+    logging.info('eval epoch: %d, loss: %s, accuracy: %s',
                  epoch, loss, accuracy * 100)
   return optimizer
 
diff --git a/examples/mnist/train_test.py b/examples/mnist/train_test.py
index eee3c3d..4d779f2 100644
--- a/examples/mnist/train_test.py
+++ b/examples/mnist/train_test.py
@@ -33,8 +33,7 @@ class TrainTest(absltest.TestCase):
   def test_train_one_epoch(self):
     train_ds, test_ds = train.get_datasets()
     input_rng = onp.random.RandomState(0)
-    model = train.create_model(random.PRNGKey(0))
-    optimizer = train.create_optimizer(model, 0.1, 0.9)
+    optimizer = train.create_optimizer(random.split(random.PRNGKey(0), 1))
     optimizer, train_metrics = train.train_epoch(optimizer, train_ds, 128, 0,
                                                  input_rng)
     self.assertLessEqual(train_metrics['loss'], 0.27)
