diff --git a/examples/mnist/train.py b/examples/mnist/train.py
index 5dc74db..5ba4df4 100644
--- a/examples/mnist/train.py
+++ b/examples/mnist/train.py
@@ -23,6 +23,9 @@ from absl import app
 from absl import flags
 from absl import logging
 
+import functools
+
+from flax import jax_utils
 from flax import nn
 from flax import optim
 from flax.metrics import tensorboard
@@ -79,12 +82,14 @@ class CNN(nn.Module):
     return x
 
 
+@jax.pmap
 def create_model(key):
   _, initial_params = CNN.init_by_shape(key, [((1, 28, 28, 1), jnp.float32)])
   model = nn.Model(CNN, initial_params)
   return model
 
 
+@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2))
 def create_optimizer(model, learning_rate, beta):
   optimizer_def = optim.Momentum(learning_rate=learning_rate, beta=beta)
   optimizer = optimizer_def.create(model)
@@ -110,7 +115,7 @@ def compute_metrics(logits, labels):
   return metrics
 
 
-@jax.jit
+@jax.pmap
 def train_step(optimizer, batch):
   """Train for a single step."""
   def loss_fn(model):
@@ -124,7 +129,7 @@ def train_step(optimizer, batch):
   return optimizer, metrics
 
 
-@jax.jit
+@jax.pmap
 def eval_step(model, batch):
   logits = model(batch['image'])
   return compute_metrics(logits, batch['label'])
@@ -141,16 +146,23 @@ def train_epoch(optimizer, train_ds, batch_size, epoch, rng):
   batch_metrics = []
   for perm in perms:
     batch = {k: v[perm] for k, v in train_ds.items()}
+    batch = jax_utils.replicate(batch)
     optimizer, metrics = train_step(optimizer, batch)
     batch_metrics.append(metrics)
 
   # compute mean of metrics across each batch in epoch.
   batch_metrics_np = jax.device_get(batch_metrics)
+  # stack all of the metrics from each devioce into
+  # numpy arrays directly on a dict, such that, e.g.
+  # `batch_metrics_np['loss']` has shape (jax.device_count(), )
+  batch_metrics_np = jax.tree_multimap(lambda *xs: onp.array(xs),
+                                       *batch_metrics_np)
   epoch_metrics_np = {
-      k: onp.mean([metrics[k] for metrics in batch_metrics_np])
-      for k in batch_metrics_np[0]}
-
-  logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,
+      k: onp.mean(batch_metrics_np[k], axis=0)
+      for k in batch_metrics_np
+  }
+  # `epoch_metrics_np` now contains 1D arrays of length `jax.device_count()`
+  logging.info('train epoch: %d, loss: %s, accuracy: %s', epoch,
                epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100)
 
   return optimizer, epoch_metrics_np
@@ -159,7 +171,7 @@ def train_epoch(optimizer, train_ds, batch_size, epoch, rng):
 def eval_model(model, test_ds):
   metrics = eval_step(model, test_ds)
   metrics = jax.device_get(metrics)
-  summary = jax.tree_map(lambda x: x.item(), metrics)
+  summary = metrics
   return summary['loss'], summary['accuracy']
 
 
@@ -184,16 +196,19 @@ def train(train_ds, test_ds):
 
   summary_writer = tensorboard.SummaryWriter(model_dir)
 
+  rng = random.split(rng, jax.device_count())
   model = create_model(rng)
   optimizer = create_optimizer(model, FLAGS.learning_rate, FLAGS.momentum)
 
   input_rng = onp.random.RandomState(0)
+  test_ds = jax_utils.replicate(test_ds)
 
   for epoch in range(1, num_epochs + 1):
     optimizer, train_metrics = train_epoch(
         optimizer, train_ds, batch_size, epoch, input_rng)
     loss, accuracy = eval_model(optimizer.target, test_ds)
-    logging.info('eval epoch: %d, loss: %.4f, accuracy: %.2f',
+    # `loss` and `accuracy` are now 1D arrays of length `jax.device_count()`
+    logging.info('eval epoch: %d, loss: %s, accuracy: %s',
                  epoch, loss, accuracy * 100)
     summary_writer.scalar('train_loss', train_metrics['loss'], epoch)
     summary_writer.scalar('train_accuracy', train_metrics['accuracy'], epoch)
diff --git a/examples/mnist/train_test.py b/examples/mnist/train_test.py
index 429bda4..c1405d0 100644
--- a/examples/mnist/train_test.py
+++ b/examples/mnist/train_test.py
@@ -21,6 +21,8 @@ import train
 import jax
 from jax import random
 
+from flax import jax_utils
+
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
@@ -30,20 +32,23 @@ class TrainTest(absltest.TestCase):
   def test_single_train_step(self):
     train_ds, test_ds = train.get_datasets()
     batch_size = 32
-    model = train.create_model(random.PRNGKey(0))
+    model = train.create_model(random.split(random.PRNGKey(0),
+                                            jax.device_count()))
+    test_ds = jax_utils.replicate(test_ds)
     optimizer = train.create_optimizer(model, 0.1, 0.9)
 
     # test single train step.
+    batch = jax_utils.replicate({k: v[:batch_size] for k, v in train_ds.items()})
     optimizer, train_metrics = train.train_step(
         optimizer=optimizer,
-        batch={k: v[:batch_size] for k, v in train_ds.items()})
-    self.assertLessEqual(train_metrics['loss'], 2.302)
-    self.assertGreaterEqual(train_metrics['accuracy'], 0.0625)
+        batch=batch)
+    self.assertLessEqual(train_metrics['loss'], 2.34)
+    self.assertGreaterEqual(train_metrics['accuracy'], 0.03125)
 
     # Run eval model.
     loss, accuracy = train.eval_model(optimizer.target, test_ds)
-    self.assertLess(loss, 2.252)
-    self.assertGreater(accuracy, 0.2597)
+    self.assertLessEqual(train_metrics['loss'], 2.34)
+    self.assertGreaterEqual(train_metrics['accuracy'], 0.03125)
 
 if __name__ == '__main__':
   absltest.main()
